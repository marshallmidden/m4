diff --no-dereference -ru ORIG.linux/block/partition-generic.c linux/block/partition-generic.c
--- ORIG.linux/block/partition-generic.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/block/partition-generic.c	2022-03-23 15:20:23.063143960 -0500
@@ -631,6 +631,7 @@
 			continue;
 		}
 #ifdef CONFIG_BLK_DEV_MD
+pr_err("%s:%u:%s %s%d flags=0x%02x ADDPART_FLAG_RAID=0x%02x calling md_autodetect_dev\n", __FILE__,__LINE__,__func__, disk->disk_name, p, state->parts[p].flags, ADDPART_FLAG_RAID);
 		if (state->parts[p].flags & ADDPART_FLAG_RAID)
 			md_autodetect_dev(part_to_dev(part)->devt);
 #endif
diff --no-dereference -ru ORIG.linux/block/partitions/efi.c linux/block/partitions/efi.c
--- ORIG.linux/block/partitions/efi.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/block/partitions/efi.c	2022-03-28 16:18:02.095828503 -0500
@@ -705,7 +705,10 @@
 
 		/* If this is a RAID volume, tell md */
 		if (!efi_guidcmp(ptes[i].partition_type_guid, PARTITION_LINUX_RAID_GUID))
+{
+pr_err("%s:%u:%s flags=0x%02x set ADDPART_FLAG_RAID=0x%02x\n", __FILE__,__LINE__,__func__, state->parts[i+1].flags, ADDPART_FLAG_RAID);
 			state->parts[i + 1].flags = ADDPART_FLAG_RAID;
+}
 
 		info = &state->parts[i + 1].info;
 		efi_guid_to_str(&ptes[i].unique_partition_guid, info->uuid);
diff --no-dereference -ru ORIG.linux/block/partitions/mac.c linux/block/partitions/mac.c
--- ORIG.linux/block/partitions/mac.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/block/partitions/mac.c	2022-03-28 16:18:36.329904640 -0500
@@ -87,7 +87,10 @@
 			be32_to_cpu(part->block_count) * (secsize/512));
 
 		if (!strncasecmp(part->type, "Linux_RAID", 10))
+{
+pr_err("%s:%u:%s flags=0x%02x set ADDPART_FLAG_RAID=0x%02x\n", __FILE__,__LINE__,__func__, state->parts[slot].flags, ADDPART_FLAG_RAID);
 			state->parts[slot].flags = ADDPART_FLAG_RAID;
+}
 #ifdef CONFIG_PPC_PMAC
 		/*
 		 * If this is the first bootable partition, tell the
diff --no-dereference -ru ORIG.linux/block/partitions/msdos.c linux/block/partitions/msdos.c
--- ORIG.linux/block/partitions/msdos.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/block/partitions/msdos.c	2022-03-28 16:19:55.705081175 -0500
@@ -183,7 +183,10 @@
 			put_partition(state, state->next, next, size);
 			set_info(state, state->next, disksig);
 			if (SYS_IND(p) == LINUX_RAID_PARTITION)
+{
+pr_err("%s:%u:%s flags=0x%02x set ADDPART_FLAG_RAID=0x%02x\n", __FILE__,__LINE__,__func__, state->parts[state->next].flags, ADDPART_FLAG_RAID);
 				state->parts[state->next].flags = ADDPART_FLAG_RAID;
+}
 			loopct = 0;
 			if (++state->next == state->limit)
 				goto done;
@@ -556,7 +559,10 @@
 		put_partition(state, slot, start, size);
 		set_info(state, slot, disksig);
 		if (SYS_IND(p) == LINUX_RAID_PARTITION)
+{
+pr_err("%s:%u:%s flags=0x%02x set ADDPART_FLAG_RAID=0x%02x\n", __FILE__,__LINE__,__func__, state->parts[slot].flags, ADDPART_FLAG_RAID);
 			state->parts[slot].flags = ADDPART_FLAG_RAID;
+}
 		if (SYS_IND(p) == DM6_PARTITION)
 			strlcat(state->pp_buf, "[DM]", PAGE_SIZE);
 		if (SYS_IND(p) == EZD_PARTITION)
diff --no-dereference -ru ORIG.linux/block/partitions/sgi.c linux/block/partitions/sgi.c
--- ORIG.linux/block/partitions/sgi.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/block/partitions/sgi.c	2022-03-28 16:20:15.089124289 -0500
@@ -73,7 +73,10 @@
 		if (blocks) {
 			put_partition(state, slot, start, blocks);
 			if (be32_to_cpu(p->type) == LINUX_RAID_PARTITION)
+{
+pr_err("%s:%u:%s flags=0x%02x set ADDPART_FLAG_RAID=0x%02x\n", __FILE__,__LINE__,__func__, state->parts[slot].flags, ADDPART_FLAG_RAID);
 				state->parts[slot].flags = ADDPART_FLAG_RAID;
+}
 		}
 		slot++;
 	}
diff --no-dereference -ru ORIG.linux/block/partitions/sun.c linux/block/partitions/sun.c
--- ORIG.linux/block/partitions/sun.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/block/partitions/sun.c	2022-03-28 16:20:43.648187806 -0500
@@ -110,7 +110,10 @@
 			state->parts[slot].flags = 0;
 			if (use_vtoc) {
 				if (be16_to_cpu(label->vtoc.infos[i].id) == LINUX_RAID_PARTITION)
+{
+pr_err("%s:%u:%s flags=0x%02x set ADDPART_FLAG_RAID=0x%02x\n", __FILE__,__LINE__,__func__, state->parts[slot].flags, ADDPART_FLAG_RAID);
 					state->parts[slot].flags |= ADDPART_FLAG_RAID;
+}
 				else if (be16_to_cpu(label->vtoc.infos[i].id) == SUN_WHOLE_DISK)
 					state->parts[slot].flags |= ADDPART_FLAG_WHOLEDISK;
 			}
diff --no-dereference -ru ORIG.linux/drivers/md/dm-raid1.c linux/drivers/md/dm-raid1.c
--- ORIG.linux/drivers/md/dm-raid1.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/drivers/md/dm-raid1.c	2022-03-18 16:31:26.235321133 -0500
@@ -220,6 +220,7 @@
 	 * simple way to tell if a device has encountered
 	 * errors.
 	 */
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	atomic_inc(&m->error_count);
 
 	if (test_and_set_bit(error_type, &m->error_type))
@@ -300,6 +301,7 @@
 	struct mirror_set *ms = dm_rh_region_context(reg);
 	int m, bit = 0;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	if (read_err) {
 		/* Read error means the failure of default mirror. */
 		DMERR_LIMIT("Unable to read primary mirror during recovery");
@@ -374,6 +376,7 @@
 {
 	unsigned int m;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	ms->leg_failure = 0;
 	for (m = 0; m < ms->nr_mirrors; m++) {
 		atomic_set(&(ms->mirror[m].error_count), 0);
@@ -416,6 +419,7 @@
 {
 	struct mirror *m = get_default_mirror(ms);
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	do {
 		if (likely(!atomic_read(&m->error_count)))
 			return m;
@@ -431,6 +435,7 @@
 {
 	struct mirror *default_mirror = get_default_mirror(m->ms);
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	return !atomic_read(&default_mirror->error_count);
 }
 
@@ -562,6 +567,7 @@
 	struct bio *bio;
 	struct mirror *m;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	while ((bio = bio_list_pop(reads))) {
 		region = dm_rh_bio_to_region(ms->rh, bio);
 		m = get_default_mirror(ms);
@@ -943,6 +949,7 @@
 	char dummy;
 	int ret;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	if (sscanf(argv[1], "%llu%c", &offset, &dummy) != 1 ||
 	    offset != (sector_t)offset) {
 		ti->error = "Invalid offset";
@@ -1383,6 +1390,7 @@
  */
 static char device_status_char(struct mirror *m)
 {
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	if (!atomic_read(&(m->error_count)))
 		return 'A';
 
diff --no-dereference -ru ORIG.linux/drivers/md/dm-sysfs.c linux/drivers/md/dm-sysfs.c
--- ORIG.linux/drivers/md/dm-sysfs.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/drivers/md/dm-sysfs.c	2022-03-18 16:28:33.815929319 -0500
@@ -26,6 +26,7 @@
 	struct mapped_device *md;
 	ssize_t ret;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	dm_attr = container_of(attr, struct dm_sysfs_attr, attr);
 	if (!dm_attr->show)
 		return -EIO;
@@ -34,6 +35,7 @@
 	if (!md)
 		return -EINVAL;
 
+pr_err("%s:%u:%s calling dm_attr->show",__FILE__,__LINE__,__func__);
 	ret = dm_attr->show(md, page);
 	dm_put(md);
 
diff --no-dereference -ru ORIG.linux/drivers/md/md.c linux/drivers/md/md.c
--- ORIG.linux/drivers/md/md.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/drivers/md/md.c	2022-04-20 18:48:54.000567989 -0500
@@ -115,18 +115,21 @@
 static int sysctl_speed_limit_max = 200000;
 static inline int speed_min(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return mddev->sync_speed_min ?
 		mddev->sync_speed_min : sysctl_speed_limit_min;
 }
 
 static inline int speed_max(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return mddev->sync_speed_max ?
 		mddev->sync_speed_max : sysctl_speed_limit_max;
 }
 
 static int rdev_init_wb(struct md_rdev *rdev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (rdev->bdev->bd_queue->nr_hw_queues == 1)
 		return 0;
 
@@ -145,6 +148,7 @@
 void mddev_create_wb_pool(struct mddev *mddev, struct md_rdev *rdev,
 			  bool is_suspend)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->bitmap_info.max_write_behind == 0)
 		return;
 
@@ -173,6 +177,7 @@
  */
 static void mddev_destroy_wb_pool(struct mddev *mddev, struct md_rdev *rdev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!test_and_clear_bit(WBCollisionCheck, &rdev->flags))
 		return;
 
@@ -253,6 +258,7 @@
 struct bio *bio_alloc_mddev(gfp_t gfp_mask, int nr_iovecs,
 			    struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev || !bioset_initialized(&mddev->bio_set))
 		return bio_alloc(gfp_mask, nr_iovecs);
 
@@ -262,6 +268,7 @@
 
 static struct bio *md_bio_alloc_sync(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev || !bioset_initialized(&mddev->sync_set))
 		return bio_alloc(GFP_NOIO, 1);
 
@@ -282,6 +289,7 @@
 static atomic_t md_event_count;
 void md_new_event(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	atomic_inc(&md_event_count);
 	wake_up(&md_event_waiters);
 }
@@ -325,6 +333,7 @@
  */
 static bool is_suspended(struct mddev *mddev, struct bio *bio)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->suspended)
 		return true;
 	if (bio_data_dir(bio) != WRITE)
@@ -340,6 +349,7 @@
 
 void md_handle_request(struct mddev *mddev, struct bio *bio)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 check_suspended:
 	rcu_read_lock();
 	if (is_suspended(mddev, bio)) {
@@ -397,6 +407,7 @@
 	/* bio could be mergeable after passing to underlayer */
 	bio->bi_opf &= ~REQ_NOMERGE;
 
+// pr_err("%s:%u:%s %s calling md_handle_request",__FILE__,__LINE__,__func__,mdname(mddev));
 	md_handle_request(mddev, bio);
 
 	part_stat_lock();
@@ -416,6 +427,7 @@
 void mddev_suspend(struct mddev *mddev)
 {
 	WARN_ON_ONCE(mddev->thread && current == mddev->thread->tsk);
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	lockdep_assert_held(&mddev->reconfig_mutex);
 	if (mddev->suspended++)
 		return;
@@ -435,6 +447,7 @@
 void mddev_resume(struct mddev *mddev)
 {
 	lockdep_assert_held(&mddev->reconfig_mutex);
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (--mddev->suspended)
 		return;
 	wake_up(&mddev->sb_wait);
@@ -451,6 +464,7 @@
 	struct md_personality *pers = mddev->pers;
 	int ret = 0;
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	rcu_read_lock();
 	if (mddev->suspended)
 		ret = 1;
@@ -463,6 +477,7 @@
 static int md_congested(void *data, int bits)
 {
 	struct mddev *mddev = data;
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return mddev_congested(mddev, bits);
 }
 
@@ -475,6 +490,7 @@
 	struct md_rdev *rdev = bio->bi_private;
 	struct mddev *mddev = rdev->mddev;
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	rdev_dec_pending(rdev, mddev);
 
 	if (atomic_dec_and_test(&mddev->flush_pending)) {
@@ -491,6 +507,7 @@
 	struct mddev *mddev = container_of(ws, struct mddev, flush_work);
 	struct md_rdev *rdev;
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	mddev->start_flush = ktime_get_boottime();
 	INIT_WORK(&mddev->flush_work, md_submit_flush_data);
 	atomic_set(&mddev->flush_pending, 1);
@@ -532,6 +549,7 @@
 	 * could wait for this and below md_handle_request could wait for those
 	 * bios because of suspend check
 	 */
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	mddev->last_flush = mddev->start_flush;
 	mddev->flush_bio = NULL;
 	wake_up(&mddev->sb_wait);
@@ -548,6 +566,7 @@
 void md_flush_request(struct mddev *mddev, struct bio *bio)
 {
 	ktime_t start = ktime_get_boottime();
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	spin_lock_irq(&mddev->lock);
 	wait_event_lock_irq(mddev->sb_wait,
 			    !mddev->flush_bio ||
@@ -578,6 +597,7 @@
 
 static inline struct mddev *mddev_get(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	atomic_inc(&mddev->active);
 	return mddev;
 }
@@ -586,6 +606,7 @@
 
 static void mddev_put(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!atomic_dec_and_lock(&mddev->active, &all_mddevs_lock))
 		return;
 	if (!mddev->raid_disks && list_empty(&mddev->disks) &&
@@ -634,7 +655,7 @@
 
 static struct mddev *mddev_find(dev_t unit)
 {
-	struct mddev *mddev, *new = NULL;
+	struct mddev *mddev=NULL, *new = NULL;
 
 	if (unit && MAJOR(unit) != MD_MAJOR)
 		unit &= ~((1<<MdpMinorShift)-1);
@@ -648,6 +669,7 @@
 				mddev_get(mddev);
 				spin_unlock(&all_mddevs_lock);
 				kfree(new);
+pr_err("%s:%u:%s %s returning old",__FILE__,__LINE__,__func__,mdname(mddev));
 				return mddev;
 			}
 
@@ -655,6 +677,7 @@
 			list_add(&new->all_mddevs, &all_mddevs);
 			spin_unlock(&all_mddevs_lock);
 			new->hold_active = UNTIL_IOCTL;
+pr_err("%s:%u:%s %s returning new",__FILE__,__LINE__,__func__,mdname(new));
 			return new;
 		}
 	} else if (new) {
@@ -672,6 +695,7 @@
 				/* Oh dear, all in use. */
 				spin_unlock(&all_mddevs_lock);
 				kfree(new);
+pr_err("%s:%u:%s no md returning NULL",__FILE__,__LINE__,__func__);
 				return NULL;
 			}
 
@@ -687,13 +711,17 @@
 		new->hold_active = UNTIL_STOP;
 		list_add(&new->all_mddevs, &all_mddevs);
 		spin_unlock(&all_mddevs_lock);
+pr_err("%s:%u:%s %s returning new",__FILE__,__LINE__,__func__,mdname(new));
 		return new;
 	}
 	spin_unlock(&all_mddevs_lock);
 
 	new = kzalloc(sizeof(*new), GFP_KERNEL);
 	if (!new)
+{
+pr_err("%s:%u:%s %s returning old",__FILE__,__LINE__,__func__,mdname(mddev));
 		return NULL;
+}
 
 	new->unit = unit;
 	if (MAJOR(unit) == MD_MAJOR)
@@ -710,6 +738,7 @@
 
 void mddev_unlock(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->to_remove) {
 		/* These cannot be removed under reconfig_mutex as
 		 * an access to the files will try to take reconfig_mutex
@@ -757,6 +786,7 @@
 {
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	rdev_for_each_rcu(rdev, mddev)
 		if (rdev->desc_nr == nr)
 			return rdev;
@@ -769,6 +799,7 @@
 {
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	rdev_for_each(rdev, mddev)
 		if (rdev->bdev->bd_dev == dev)
 			return rdev;
@@ -780,6 +811,7 @@
 {
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	rdev_for_each_rcu(rdev, mddev)
 		if (rdev->bdev->bd_dev == dev)
 			return rdev;
@@ -809,6 +841,7 @@
 
 static int alloc_disk_sb(struct md_rdev *rdev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	rdev->sb_page = alloc_page(GFP_KERNEL);
 	if (!rdev->sb_page)
 		return -ENOMEM;
@@ -817,6 +850,7 @@
 
 void md_rdev_clear(struct md_rdev *rdev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (rdev->sb_page) {
 		put_page(rdev->sb_page);
 		rdev->sb_loaded = 0;
@@ -837,8 +871,9 @@
 	struct md_rdev *rdev = bio->bi_private;
 	struct mddev *mddev = rdev->mddev;
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (bio->bi_status) {
-		pr_err("md: super_written gets error=%d\n", bio->bi_status);
+		pr_err("md: super_written gets error=%d", bio->bi_status);
 		md_error(mddev, rdev);
 		if (!test_bit(Faulty, &rdev->flags)
 		    && (bio->bi_opf & MD_FAILFAST)) {
@@ -866,6 +901,7 @@
 	struct bio *bio;
 	int ff = 0;
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!page)
 		return;
 
@@ -895,6 +931,7 @@
 int md_super_wait(struct mddev *mddev)
 {
 	/* wait for all superblock writes that were scheduled to complete */
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	wait_event(mddev->sb_wait, atomic_read(&mddev->pending_writes)==0);
 	if (test_and_clear_bit(MD_SB_NEED_REWRITE, &mddev->sb_flags))
 		return -EAGAIN;
@@ -907,6 +944,7 @@
 	struct bio *bio = md_bio_alloc_sync(rdev->mddev);
 	int ret;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (metadata_op && rdev->meta_bdev)
 		bio_set_dev(bio, rdev->meta_bdev);
 	else
@@ -934,6 +972,7 @@
 {
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (rdev->sb_loaded)
 		return 0;
 
@@ -1077,6 +1116,7 @@
  */
 int md_check_no_bitmap(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->bitmap_info.file && !mddev->bitmap_info.offset)
 		return 0;
 	pr_warn("%s: bitmaps are not supported for %s\n",
@@ -1094,6 +1134,7 @@
 	mdp_super_t *sb;
 	int ret;
 
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 	/*
 	 * Calculate the position of the superblock (512byte sectors),
 	 * it's at the end of the disk.
@@ -1191,6 +1232,7 @@
 	__u64 ev1 = md_event(sb);
 
 	rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 	clear_bit(Faulty, &rdev->flags);
 	clear_bit(In_sync, &rdev->flags);
 	clear_bit(Bitmap_sync, &rdev->flags);
@@ -1289,6 +1331,7 @@
 			    desc->raid_disk < mddev->raid_disks */) {
 			set_bit(In_sync, &rdev->flags);
 			rdev->raid_disk = desc->raid_disk;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			rdev->saved_raid_disk = desc->raid_disk;
 		} else if (desc->state & (1<<MD_DISK_ACTIVE)) {
 			/* active but not in sync implies recovery up to
@@ -1297,6 +1340,7 @@
 			if (mddev->minor_version >= 91) {
 				rdev->recovery_offset = 0;
 				rdev->raid_disk = desc->raid_disk;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			}
 		}
 		if (desc->state & (1<<MD_DISK_WRITEMOSTLY))
@@ -1330,6 +1374,7 @@
 	int i;
 	int active=0, working=0,failed=0,spare=0,nr_disks=0;
 
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 	rdev->sb_size = MD_SB_BYTES;
 
 	sb = page_address(rdev->sb_page);
@@ -1413,6 +1458,7 @@
 			d->raid_disk = rdev2->raid_disk;
 		else
 			d->raid_disk = rdev2->desc_nr; /* compatibility */
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),d->raid_disk);
 		if (test_bit(Faulty, &rdev2->flags))
 			d->state = (1<<MD_DISK_FAULTY);
 		else if (is_active) {
@@ -1437,6 +1483,7 @@
 		if (d->state == 0 && d->number == 0) {
 			d->number = i;
 			d->raid_disk = i;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),d->raid_disk);
 			d->state = (1<<MD_DISK_REMOVED);
 			d->state |= (1<<MD_DISK_FAULTY);
 			failed++;
@@ -1458,6 +1505,7 @@
 static unsigned long long
 super_90_rdev_size_change(struct md_rdev *rdev, sector_t num_sectors)
 {
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 	if (num_sectors && num_sectors < rdev->mddev->dev_sectors)
 		return 0; /* component must fit device */
 	if (rdev->mddev->bitmap_info.offset)
@@ -1519,6 +1567,7 @@
 	char b[BDEVNAME_SIZE], b2[BDEVNAME_SIZE];
 	int bmask;
 
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 	/*
 	 * Calculate the position of the superblock in 512byte sectors.
 	 * It is always aligned to a 4K boundary and
@@ -1687,6 +1736,7 @@
 	__u64 ev1 = le64_to_cpu(sb->events);
 
 	rdev->raid_disk = -1;
+pr_err("%s:%u:%s #0 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 	clear_bit(Faulty, &rdev->flags);
 	clear_bit(In_sync, &rdev->flags);
 	clear_bit(Bitmap_sync, &rdev->flags);
@@ -1764,11 +1814,17 @@
 		    (MD_FEATURE_PPL | MD_FEATURE_MULTIPLE_PPLS)) {
 			if (le32_to_cpu(sb->feature_map) &
 			    (MD_FEATURE_BITMAP_OFFSET | MD_FEATURE_JOURNAL))
+{
+pr_err("%s:%u:%s #1 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 				return -EINVAL;
+}
 			if ((le32_to_cpu(sb->feature_map) & MD_FEATURE_PPL) &&
 			    (le32_to_cpu(sb->feature_map) &
 					    MD_FEATURE_MULTIPLE_PPLS))
+{
+pr_err("%s:%u:%s #2 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 				return -EINVAL;
+}
 			set_bit(MD_HAS_PPL, &mddev->flags);
 		}
 	} else if (mddev->pers == NULL) {
@@ -1780,19 +1836,28 @@
 		    (le16_to_cpu(sb->dev_roles[rdev->desc_nr]) < MD_DISK_ROLE_MAX ||
 		     le16_to_cpu(sb->dev_roles[rdev->desc_nr]) == MD_DISK_ROLE_JOURNAL))
 			if (ev1 < mddev->events)
+{
+pr_err("%s:%u:%s #3 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 				return -EINVAL;
+}
 	} else if (mddev->bitmap) {
 		/* If adding to array with a bitmap, then we can accept an
 		 * older device, but not too old.
 		 */
 		if (ev1 < mddev->bitmap->events_cleared)
+{
+pr_err("%s:%u:%s #4 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			return 0;
+}
 		if (ev1 < mddev->events)
 			set_bit(Bitmap_sync, &rdev->flags);
 	} else {
 		if (ev1 < mddev->events)
 			/* just a hot-add of a new device, leave raid_disk at -1 */
+{
+pr_err("%s:%u:%s #5 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			return 0;
+}
 	}
 	if (mddev->level != LEVEL_MULTIPATH) {
 		int role;
@@ -1812,11 +1877,13 @@
 			if (!(le32_to_cpu(sb->feature_map) & MD_FEATURE_JOURNAL)) {
 				/* journal device without journal feature */
 				pr_warn("md: journal device provided without journal feature, ignoring the device\n");
+pr_err("%s:%u:%s #6 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 				return -EINVAL;
 			}
 			set_bit(Journal, &rdev->flags);
 			rdev->journal_tail = le64_to_cpu(sb->journal_tail);
 			rdev->raid_disk = 0;
+pr_err("%s:%u:%s #7 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			break;
 		default:
 			rdev->saved_raid_disk = role;
@@ -1829,6 +1896,7 @@
 			} else
 				set_bit(In_sync, &rdev->flags);
 			rdev->raid_disk = role;
+pr_err("%s:%u:%s #8 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			break;
 		}
 		if (sb->devflags & WriteMostly1)
@@ -1840,6 +1908,7 @@
 	} else /* MULTIPATH are always insync */
 		set_bit(In_sync, &rdev->flags);
 
+pr_err("%s:%u:%s #9 %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 	return 0;
 }
 
@@ -1850,6 +1919,7 @@
 	int max_dev, i;
 	/* make rdev->sb match mddev and rdev data. */
 
+// pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 	sb = page_address(rdev->sb_page);
 
 	sb->feature_map = 0;
@@ -2016,6 +2086,7 @@
 {
 	struct mdp_superblock_1 *sb;
 	sector_t max_sectors;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 	if (num_sectors && num_sectors < rdev->mddev->dev_sectors)
 		return 0; /* component must fit device */
 	if (rdev->data_offset != rdev->new_data_offset)
@@ -2057,6 +2128,7 @@
 {
 	/* All necessary checks on new >= old have been done */
 	struct bitmap *bitmap;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (new_offset >= rdev->data_offset)
 		return 1;
 
@@ -2107,6 +2179,7 @@
 
 static void sync_super(struct mddev *mddev, struct md_rdev *rdev)
 {
+// pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 	if (mddev->sync_super) {
 		mddev->sync_super(mddev, rdev);
 		return;
@@ -2126,12 +2199,18 @@
 		if (test_bit(Faulty, &rdev->flags) ||
 		    test_bit(Journal, &rdev->flags) ||
 		    rdev->raid_disk == -1)
+{
+pr_err("%s:%u:%s %s/%s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev1),mdname(mddev2),rdev->raid_disk);
 			continue;
+}
 		rdev_for_each_rcu(rdev2, mddev2) {
 			if (test_bit(Faulty, &rdev2->flags) ||
 			    test_bit(Journal, &rdev2->flags) ||
 			    rdev2->raid_disk == -1)
+{
+pr_err("%s:%u:%s %s/%s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev1),mdname(mddev2),rdev->raid_disk);
 				continue;
+}
 			if (rdev->bdev->bd_contains ==
 			    rdev2->bdev->bd_contains) {
 				rcu_read_unlock();
@@ -2156,11 +2235,13 @@
 {
 	struct md_rdev *rdev, *reference = NULL;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (list_empty(&mddev->disks))
 		return 0; /* nothing to do */
 	if (!mddev->gendisk || blk_get_integrity(mddev->gendisk))
 		return 0; /* shouldn't register, or already is */
 	rdev_for_each(rdev, mddev) {
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 		/* skip spares and non-functional disks */
 		if (test_bit(Faulty, &rdev->flags))
 			continue;
@@ -2204,6 +2285,7 @@
 	struct blk_integrity *bi_mddev;
 	char name[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->gendisk)
 		return 0;
 
@@ -2228,13 +2310,20 @@
 	struct kobject *ko;
 	int err;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* prevent duplicates */
 	if (find_rdev(mddev, rdev->bdev->bd_dev))
+{
+pr_err("%s:%u:%s %s -EEXIST",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EEXIST;
+}
 
 	if ((bdev_read_only(rdev->bdev) || bdev_read_only(rdev->meta_bdev)) &&
 	    mddev->pers)
+{
+pr_err("%s:%u:%s %s -EROFS",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EROFS;
+}
 
 	/* make sure rdev->sectors exceeds mddev->dev_sectors */
 	if (!test_bit(Journal, &rdev->flags) &&
@@ -2246,7 +2335,10 @@
 			 * about aligning sizes (e.g. linear)
 			 */
 			if (mddev->level > 0)
+{
+pr_err("%s:%u:%s %s -ENOSPC",__FILE__,__LINE__,__func__,mdname(mddev));
 				return -ENOSPC;
+}
 		} else
 			mddev->dev_sectors = rdev->sectors;
 	}
@@ -2266,6 +2358,7 @@
 	} else {
 		if (md_find_rdev_nr_rcu(mddev, rdev->desc_nr)) {
 			rcu_read_unlock();
+pr_err("%s:%u:%s %s #1 -EBUSY",__FILE__,__LINE__,__func__,mdname(mddev));
 			return -EBUSY;
 		}
 	}
@@ -2274,6 +2367,7 @@
 	    mddev->max_disks && rdev->desc_nr >= mddev->max_disks) {
 		pr_warn("md: %s: array is limited to %d devices\n",
 			mdname(mddev), mddev->max_disks);
+pr_err("%s:%u:%s %s #2 -EBUSY",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EBUSY;
 	}
 	bdevname(rdev->bdev,b);
@@ -2299,11 +2393,13 @@
 	/* May as well allow recovery to be retried once */
 	mddev->recovery_disabled++;
 
+pr_err("%s:%u:%s %s 0",__FILE__,__LINE__,__func__,mdname(mddev));
 	return 0;
 
  fail:
 	pr_warn("md: failed to register dev-%s for %s\n",
 		b, mdname(mddev));
+pr_err("%s:%u:%s %s err=%d",__FILE__,__LINE__,__func__,mdname(mddev),err);
 	return err;
 }
 
@@ -2318,6 +2414,7 @@
 {
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	bd_unlink_disk_holder(rdev->bdev, rdev->mddev->gendisk);
 	list_del_rcu(&rdev->same_set);
 	pr_debug("md: unbind<%s>\n", bdevname(rdev->bdev,b));
@@ -2348,6 +2445,7 @@
 	struct block_device *bdev;
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	bdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL,
 				 shared ? (struct md_rdev *)lock_rdev : rdev);
 	if (IS_ERR(bdev)) {
@@ -2361,6 +2459,7 @@
 static void unlock_rdev(struct md_rdev *rdev)
 {
 	struct block_device *bdev = rdev->bdev;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	rdev->bdev = NULL;
 	blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 }
@@ -2371,11 +2470,15 @@
 {
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s Entering %s",__FILE__,__LINE__,__func__,mdname(rdev->mddev),bdevname(rdev->bdev,b));
 	pr_debug("md: export_rdev(%s)\n", bdevname(rdev->bdev,b));
 	md_rdev_clear(rdev);
 #ifndef MODULE
 	if (test_bit(AutoDetected, &rdev->flags))
+{
+pr_err("%s:%u:%s %s Entering calling md_autodetect_dev",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 		md_autodetect_dev(rdev->bdev->bd_dev);
+}
 #endif
 	unlock_rdev(rdev);
 	kobject_put(&rdev->kobj);
@@ -2383,6 +2486,7 @@
 
 void md_kick_rdev_from_array(struct md_rdev *rdev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	unbind_rdev_from_array(rdev);
 	export_rdev(rdev);
 }
@@ -2392,6 +2496,7 @@
 {
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	while (!list_empty(&mddev->disks)) {
 		rdev = list_first_entry(&mddev->disks, struct md_rdev,
 					same_set);
@@ -2404,6 +2509,7 @@
 static bool set_in_sync(struct mddev *mddev)
 {
 	lockdep_assert_held(&mddev->lock);
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->in_sync) {
 		mddev->sync_checkers++;
 		spin_unlock(&mddev->lock);
@@ -2437,6 +2543,7 @@
 	 * with the rest of the array)
 	 */
 	struct md_rdev *rdev;
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	rdev_for_each(rdev, mddev) {
 		if (rdev->sb_events == mddev->events ||
 		    (nospares &&
@@ -2457,6 +2564,7 @@
 	struct mdp_superblock_1 *sb;
 	int role;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* Find a good rdev */
 	rdev_for_each(rdev, mddev)
 		if ((rdev->raid_disk >= 0) && !test_bit(Faulty, &rdev->flags))
@@ -2498,6 +2606,7 @@
 	int any_badblocks_changed = 0;
 	int ret = -1;
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->ro) {
 		if (force_change)
 			set_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);
@@ -2690,11 +2799,13 @@
 	int err = 0;
 	bool add_journal = test_bit(Journal, &rdev->flags);
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (!mddev->pers->hot_remove_disk || add_journal) {
 		/* If there is hot_add_disk but no hot_remove_disk
 		 * then added disks for geometry changes,
 		 * and should be added immediately.
 		 */
+pr_err("%s:%u:%s %s executing validate_super",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 		super_types[mddev->major_version].
 			validate_super(mddev, rdev);
 		if (add_journal)
@@ -2751,6 +2862,7 @@
 	size_t len = 0;
 	unsigned long flags = READ_ONCE(rdev->flags);
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (test_bit(Faulty, &flags) ||
 	    (!test_bit(ExternalBbl, &flags) &&
 	    rdev->badblocks.unacked_exist))
@@ -2804,6 +2916,7 @@
 	 *  {,-}failfast - set/clear FailFast
 	 */
 	int err = -EINVAL;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (cmd_match(buf, "faulty") && rdev->mddev->pers) {
 		md_error(rdev->mddev, rdev);
 		if (test_bit(Faulty, &rdev->flags))
@@ -2860,6 +2973,7 @@
 
 		err = 0;
 	} else if (cmd_match(buf, "insync") && rdev->raid_disk == -1) {
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 		set_bit(In_sync, &rdev->flags);
 		err = 0;
 	} else if (cmd_match(buf, "failfast")) {
@@ -2874,6 +2988,7 @@
 			clear_bit(In_sync, &rdev->flags);
 			rdev->saved_raid_disk = rdev->raid_disk;
 			rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 			err = 0;
 		}
 	} else if (cmd_match(buf, "write_error")) {
@@ -2924,6 +3039,7 @@
 			err = -EINVAL;
 		else if (test_bit(Faulty, &rdev->flags) && (rdev->raid_disk == -1) &&
 				rdev->saved_raid_disk >= 0) {
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 			/* clear_bit is performed _after_ all the devices
 			 * have their local Faulty bit cleared. If any writes
 			 * happen in the meantime in the local node, they
@@ -2955,6 +3071,7 @@
 static ssize_t
 errors_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return sprintf(page, "%d\n", atomic_read(&rdev->corrected_errors));
 }
 
@@ -2964,6 +3081,7 @@
 	unsigned int n;
 	int rv;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	rv = kstrtouint(buf, 10, &n);
 	if (rv < 0)
 		return rv;
@@ -2976,6 +3094,7 @@
 static ssize_t
 slot_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (test_bit(Journal, &rdev->flags))
 		return sprintf(page, "journal\n");
 	else if (rdev->raid_disk < 0)
@@ -2990,6 +3109,7 @@
 	int slot;
 	int err;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (test_bit(Journal, &rdev->flags))
 		return -EBUSY;
 	if (strncmp(buf, "none", 4)==0)
@@ -3007,6 +3127,7 @@
 		 * failed/spare devices.  This normally happens automatically,
 		 * but not when the metadata is externally managed.
 		 */
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 		if (rdev->raid_disk == -1)
 			return -EEXIST;
 		/* personality does all needed checks */
@@ -3038,6 +3159,7 @@
 			return -ENOSPC;
 
 		rdev->raid_disk = slot;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 		if (test_bit(In_sync, &rdev->flags))
 			rdev->saved_raid_disk = slot;
 		else
@@ -3048,6 +3170,7 @@
 			hot_add_disk(rdev->mddev, rdev);
 		if (err) {
 			rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 			return err;
 		} else
 			sysfs_notify_dirent_safe(rdev->sysfs_state);
@@ -3059,6 +3182,7 @@
 		    slot >= rdev->mddev->raid_disks + rdev->mddev->delta_disks)
 			return -ENOSPC;
 		rdev->raid_disk = slot;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 		/* assume it is working */
 		clear_bit(Faulty, &rdev->flags);
 		clear_bit(WriteMostly, &rdev->flags);
@@ -3074,6 +3198,7 @@
 static ssize_t
 offset_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return sprintf(page, "%llu\n", (unsigned long long)rdev->data_offset);
 }
 
@@ -3081,6 +3206,7 @@
 offset_store(struct md_rdev *rdev, const char *buf, size_t len)
 {
 	unsigned long long offset;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (kstrtoull(buf, 10, &offset) < 0)
 		return -EINVAL;
 	if (rdev->mddev->pers && rdev->raid_disk >= 0)
@@ -3099,6 +3225,7 @@
 
 static ssize_t new_offset_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return sprintf(page, "%llu\n",
 		       (unsigned long long)rdev->new_data_offset);
 }
@@ -3109,6 +3236,7 @@
 	unsigned long long new_offset;
 	struct mddev *mddev = rdev->mddev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (kstrtoull(buf, 10, &new_offset) < 0)
 		return -EINVAL;
 
@@ -3158,6 +3286,7 @@
 static ssize_t
 rdev_size_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return sprintf(page, "%llu\n", (unsigned long long)rdev->sectors / 2);
 }
 
@@ -3197,6 +3326,7 @@
 	sector_t oldsectors = rdev->sectors;
 	sector_t sectors;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (test_bit(Journal, &rdev->flags))
 		return -EBUSY;
 	if (strict_blocks_to_sectors(buf, &sectors) < 0)
@@ -3271,6 +3401,7 @@
 {
 	unsigned long long recovery_start = rdev->recovery_offset;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (test_bit(In_sync, &rdev->flags) ||
 	    recovery_start == MaxSector)
 		return sprintf(page, "none\n");
@@ -3282,6 +3413,7 @@
 {
 	unsigned long long recovery_start;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (cmd_match(buf, "none"))
 		recovery_start = MaxSector;
 	else if (kstrtoull(buf, 10, &recovery_start))
@@ -3315,12 +3447,14 @@
  */
 static ssize_t bb_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return badblocks_show(&rdev->badblocks, page, 0);
 }
 static ssize_t bb_store(struct md_rdev *rdev, const char *page, size_t len)
 {
 	int rv = badblocks_store(&rdev->badblocks, page, len, 0);
 	/* Maybe that ack was all we needed */
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (test_and_clear_bit(BlockedBadBlocks, &rdev->flags))
 		wake_up(&rdev->blocked_wait);
 	return rv;
@@ -3330,10 +3464,12 @@
 
 static ssize_t ubb_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return badblocks_show(&rdev->badblocks, page, 1);
 }
 static ssize_t ubb_store(struct md_rdev *rdev, const char *page, size_t len)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return badblocks_store(&rdev->badblocks, page, len, 1);
 }
 static struct rdev_sysfs_entry rdev_unack_bad_blocks =
@@ -3342,6 +3478,7 @@
 static ssize_t
 ppl_sector_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return sprintf(page, "%llu\n", (unsigned long long)rdev->ppl.sector);
 }
 
@@ -3350,6 +3487,7 @@
 {
 	unsigned long long sector;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (kstrtoull(buf, 10, &sector) < 0)
 		return -EINVAL;
 	if (sector != (sector_t)sector)
@@ -3381,6 +3519,7 @@
 static ssize_t
 ppl_size_show(struct md_rdev *rdev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return sprintf(page, "%u\n", rdev->ppl.size);
 }
 
@@ -3389,6 +3528,7 @@
 {
 	unsigned int size;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	if (kstrtouint(buf, 10, &size) < 0)
 		return -EINVAL;
 
@@ -3432,9 +3572,16 @@
 	struct md_rdev *rdev = container_of(kobj, struct md_rdev, kobj);
 
 	if (!entry->show)
+{
+pr_err("%s:%u:%s %s entry->show is NULL -EIO",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 		return -EIO;
+}
 	if (!rdev->mddev)
+{
+pr_err("%s:%u:%s %s -ENODEV",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 		return -ENODEV;
+}
+pr_err("%s:%u:%s %s calling entry->show",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 	return entry->show(rdev, page);
 }
 
@@ -3447,8 +3594,12 @@
 	ssize_t rv;
 	struct mddev *mddev = rdev->mddev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__, mdname(mddev));
 	if (!entry->store)
+{
+pr_err("%s:%u:%s %s entry->store == NULL return -EIO",__FILE__,__LINE__,__func__, mdname(mddev));
 		return -EIO;
+}
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	rv = mddev ? mddev_lock(mddev) : -ENODEV;
@@ -3456,7 +3607,10 @@
 		if (rdev->mddev == NULL)
 			rv = -ENODEV;
 		else
+{
+pr_err("%s:%u:%s %s calling entry->store",__FILE__,__LINE__,__func__, mdname(mddev));
 			rv = entry->store(rdev, page, length);
+}
 		mddev_unlock(mddev);
 	}
 	return rv;
@@ -3482,6 +3636,7 @@
 	rdev->desc_nr = -1;
 	rdev->saved_raid_disk = -1;
 	rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(rdev->mddev),rdev->raid_disk);
 	rdev->flags = 0;
 	rdev->data_offset = 0;
 	rdev->new_data_offset = 0;
@@ -3520,6 +3675,7 @@
 	struct md_rdev *rdev;
 	sector_t size;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	rdev = kzalloc(sizeof(*rdev), GFP_KERNEL);
 	if (!rdev)
 		return ERR_PTR(-ENOMEM);
@@ -3546,6 +3702,8 @@
 	}
 
 	if (super_format >= 0) {
+pr_err("%s:%u:%s calling load_super",__FILE__,__LINE__,__func__);
+// pr_err("%s:%u:%s %s calling load_super",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 		err = super_types[super_format].
 			load_super(rdev, NULL, super_minor);
 		if (err == -EINVAL) {
@@ -3581,8 +3739,12 @@
 	struct md_rdev *rdev, *freshest, *tmp;
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	freshest = NULL;
 	rdev_for_each_safe(rdev, tmp, mddev)
+{
+pr_err("%s:%u:%s calling load_super",__FILE__,__LINE__,__func__);
+// pr_err("%s:%u:%s %s calling load_super",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 		switch (super_types[mddev->major_version].
 			load_super(rdev, freshest, mddev->minor_version)) {
 		case 1:
@@ -3595,7 +3757,9 @@
 				bdevname(rdev->bdev,b));
 			md_kick_rdev_from_array(rdev);
 		}
+}
 
+pr_err("%s:%u:%s %s executing validate_super",__FILE__,__LINE__,__func__,mdname(mddev));
 	super_types[mddev->major_version].
 		validate_super(mddev, freshest);
 
@@ -3611,6 +3775,7 @@
 			continue;
 		}
 		if (rdev != freshest) {
+pr_err("%s:%u:%s %s executing validate_super",__FILE__,__LINE__,__func__,mdname(mddev));
 			if (super_types[mddev->major_version].
 			    validate_super(mddev, rdev)) {
 				pr_warn("md: kicking non-fresh %s from array!\n",
@@ -3622,11 +3787,13 @@
 		if (mddev->level == LEVEL_MULTIPATH) {
 			rdev->desc_nr = i++;
 			rdev->raid_disk = rdev->desc_nr;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			set_bit(In_sync, &rdev->flags);
 		} else if (rdev->raid_disk >=
 			    (mddev->raid_disks - min(0, mddev->delta_disks)) &&
 			   !test_bit(Journal, &rdev->flags)) {
 			rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			clear_bit(In_sync, &rdev->flags);
 		}
 	}
@@ -3676,6 +3843,7 @@
 safe_delay_show(struct mddev *mddev, char *page)
 {
 	int msec = (mddev->safemode_delay*1000)/HZ;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%d.%03d\n", msec/1000, msec%1000);
 }
 static ssize_t
@@ -3683,6 +3851,7 @@
 {
 	unsigned long msec;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev_is_clustered(mddev)) {
 		pr_warn("md: Safemode is disabled for clustered mode\n");
 		return -EINVAL;
@@ -3712,6 +3881,7 @@
 {
 	struct md_personality *p;
 	int ret;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	spin_lock(&mddev->lock);
 	p = mddev->pers;
 	if (p)
@@ -3737,6 +3907,7 @@
 	void *priv, *oldpriv;
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (slen == 0 || slen >= sizeof(clevel))
 		return -EINVAL;
 
@@ -3812,7 +3983,10 @@
 	}
 
 	rdev_for_each(rdev, mddev)
+{
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 		rdev->new_raid_disk = rdev->raid_disk;
+}
 
 	/* ->takeover must set new_* and/or delta_disks
 	 * if it succeeds, and may set them when it fails.
@@ -3888,6 +4062,7 @@
 			continue;
 		if (rdev->new_raid_disk >= mddev->raid_disks)
 			rdev->new_raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 		if (rdev->new_raid_disk == rdev->raid_disk)
 			continue;
 		sysfs_unlink_rdev(mddev, rdev);
@@ -3895,9 +4070,11 @@
 	rdev_for_each(rdev, mddev) {
 		if (rdev->raid_disk < 0)
 			continue;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 		if (rdev->new_raid_disk == rdev->raid_disk)
 			continue;
 		rdev->raid_disk = rdev->new_raid_disk;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 		if (rdev->raid_disk < 0)
 			clear_bit(In_sync, &rdev->flags);
 		else {
@@ -3934,6 +4111,7 @@
 static ssize_t
 layout_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* just a number, not meaningful for all levels */
 	if (mddev->reshape_position != MaxSector &&
 	    mddev->layout != mddev->new_layout)
@@ -3948,6 +4126,7 @@
 	unsigned int n;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = kstrtouint(buf, 10, &n);
 	if (err < 0)
 		return err;
@@ -3980,6 +4159,7 @@
 static ssize_t
 raid_disks_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->raid_disks == 0)
 		return 0;
 	if (mddev->reshape_position != MaxSector &&
@@ -3997,6 +4177,7 @@
 	unsigned int n;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = kstrtouint(buf, 10, &n);
 	if (err < 0)
 		return err;
@@ -4035,6 +4216,7 @@
 static ssize_t
 chunk_size_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->reshape_position != MaxSector &&
 	    mddev->chunk_sectors != mddev->new_chunk_sectors)
 		return sprintf(page, "%d (%d)\n",
@@ -4049,6 +4231,7 @@
 	unsigned long n;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = kstrtoul(buf, 10, &n);
 	if (err < 0)
 		return err;
@@ -4081,6 +4264,7 @@
 static ssize_t
 resync_start_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->recovery_cp == MaxSector)
 		return sprintf(page, "none\n");
 	return sprintf(page, "%llu\n", (unsigned long long)mddev->recovery_cp);
@@ -4092,6 +4276,7 @@
 	unsigned long long n;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (cmd_match(buf, "none"))
 		n = MaxSector;
 	else {
@@ -4176,6 +4361,7 @@
 {
 	enum array_state st = inactive;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers)
 		switch(mddev->ro) {
 		case 1:
@@ -4204,6 +4390,7 @@
 		else
 			st = inactive;
 	}
+pr_err("%s:%u:%s %s %s",__FILE__,__LINE__,__func__,mdname(mddev), array_states[st]);
 	return sprintf(page, "%s\n", array_states[st]);
 }
 
@@ -4218,6 +4405,7 @@
 	int err = 0;
 	enum array_state st = match_word(buf, array_states);
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers && (st == active || st == clean) && mddev->ro != 1) {
 		/* don't take reconfig_mutex when toggling between
 		 * clean and active
@@ -4264,6 +4452,7 @@
 		else {
 			mddev->ro = 1;
 			set_disk_ro(mddev->gendisk, 1);
+pr_err("%s:%u:%s %s calling do_md_run",__FILE__,__LINE__,__func__,mdname(mddev));
 			err = do_md_run(mddev);
 		}
 		break;
@@ -4279,6 +4468,7 @@
 			}
 		} else {
 			mddev->ro = 2;
+pr_err("%s:%u:%s %s calling do_md_run",__FILE__,__LINE__,__func__,mdname(mddev));
 			err = do_md_run(mddev);
 		}
 		break;
@@ -4305,6 +4495,7 @@
 		} else {
 			mddev->ro = 0;
 			set_disk_ro(mddev->gendisk, 0);
+pr_err("%s:%u:%s %s calling do_md_run",__FILE__,__LINE__,__func__,mdname(mddev));
 			err = do_md_run(mddev);
 		}
 		break;
@@ -4337,6 +4528,7 @@
 	unsigned int n;
 	int rv;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	rv = kstrtouint(buf, 10, &n);
 	if (rv < 0)
 		return rv;
@@ -4351,6 +4543,7 @@
 static ssize_t
 null_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return -EINVAL;
 }
 
@@ -4371,6 +4564,7 @@
 	struct md_rdev *rdev;
 	int err;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!*buf || *e != ':' || !e[1] || e[1] == '\n')
 		return -EINVAL;
 	minor = simple_strtoul(e+1, &e, 10);
@@ -4393,6 +4587,8 @@
 			struct md_rdev *rdev0
 				= list_entry(mddev->disks.next,
 					     struct md_rdev, same_set);
+pr_err("%s:%u:%s calling load_super",__FILE__,__LINE__,__func__);
+// pr_err("%s:%u:%s %s calling load_super",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 			err = super_types[mddev->major_version]
 				.load_super(rdev, rdev0, mddev->minor_version);
 			if (err < 0)
@@ -4427,6 +4623,7 @@
 	unsigned long chunk, end_chunk;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = mddev_lock(mddev);
 	if (err)
 		return err;
@@ -4457,6 +4654,7 @@
 static ssize_t
 size_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%llu\n",
 		(unsigned long long)mddev->dev_sectors / 2);
 }
@@ -4473,6 +4671,7 @@
 	sector_t sectors;
 	int err = strict_blocks_to_sectors(buf, &sectors);
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (err < 0)
 		return err;
 	err = mddev_lock(mddev);
@@ -4505,6 +4704,7 @@
 static ssize_t
 metadata_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->persistent)
 		return sprintf(page, "%d.%d\n",
 			       mddev->major_version, mddev->minor_version);
@@ -4525,6 +4725,7 @@
 	 * no devices attached to the array.
 	 */
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = mddev_lock(mddev);
 	if (err)
 		return err;
@@ -4585,6 +4786,7 @@
 {
 	char *type = "idle";
 	unsigned long recovery = mddev->recovery;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (test_bit(MD_RECOVERY_FROZEN, &recovery))
 		type = "frozen";
 	else if (test_bit(MD_RECOVERY_RUNNING, &recovery) ||
@@ -4609,6 +4811,7 @@
 static ssize_t
 action_store(struct mddev *mddev, const char *page, size_t len)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->pers || !mddev->pers->sync_request)
 		return -EINVAL;
 
@@ -4687,6 +4890,7 @@
 static ssize_t
 mismatch_cnt_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%llu\n",
 		       (unsigned long long)
 		       atomic64_read(&mddev->resync_mismatches));
@@ -4697,6 +4901,7 @@
 static ssize_t
 sync_min_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%d (%s)\n", speed_min(mddev),
 		       mddev->sync_speed_min ? "local": "system");
 }
@@ -4707,6 +4912,7 @@
 	unsigned int min;
 	int rv;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (strncmp(buf, "system", 6)==0) {
 		min = 0;
 	} else {
@@ -4726,6 +4932,7 @@
 static ssize_t
 sync_max_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%d (%s)\n", speed_max(mddev),
 		       mddev->sync_speed_max ? "local": "system");
 }
@@ -4736,6 +4943,7 @@
 	unsigned int max;
 	int rv;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (strncmp(buf, "system", 6)==0) {
 		max = 0;
 	} else {
@@ -4755,6 +4963,7 @@
 static ssize_t
 degraded_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%d\n", mddev->degraded);
 }
 static struct md_sysfs_entry md_degraded = __ATTR_RO(degraded);
@@ -4762,6 +4971,7 @@
 static ssize_t
 sync_force_parallel_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%d\n", mddev->parallel_resync);
 }
 
@@ -4770,6 +4980,7 @@
 {
 	long n;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (kstrtol(buf, 10, &n))
 		return -EINVAL;
 
@@ -4793,6 +5004,7 @@
 sync_speed_show(struct mddev *mddev, char *page)
 {
 	unsigned long resync, dt, db;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->curr_resync == 0)
 		return sprintf(page, "none\n");
 	resync = mddev->curr_mark_cnt - atomic_read(&mddev->recovery_active);
@@ -4809,6 +5021,7 @@
 {
 	unsigned long long max_sectors, resync;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))
 		return sprintf(page, "none\n");
 
@@ -4832,6 +5045,7 @@
 static ssize_t
 min_sync_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%llu\n",
 		       (unsigned long long)mddev->resync_min);
 }
@@ -4841,6 +5055,7 @@
 	unsigned long long min;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (kstrtoull(buf, 10, &min))
 		return -EINVAL;
 
@@ -4868,6 +5083,7 @@
 static ssize_t
 max_sync_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->resync_max == MaxSector)
 		return sprintf(page, "max\n");
 	else
@@ -4878,6 +5094,7 @@
 max_sync_store(struct mddev *mddev, const char *buf, size_t len)
 {
 	int err;
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	spin_lock(&mddev->lock);
 	if (strncmp(buf, "max", 3) == 0)
 		mddev->resync_max = MaxSector;
@@ -4921,6 +5138,7 @@
 static ssize_t
 suspend_lo_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%llu\n", (unsigned long long)mddev->suspend_lo);
 }
 
@@ -4930,6 +5148,7 @@
 	unsigned long long new;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = kstrtoull(buf, 10, &new);
 	if (err < 0)
 		return err;
@@ -4958,6 +5177,7 @@
 static ssize_t
 suspend_hi_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%llu\n", (unsigned long long)mddev->suspend_hi);
 }
 
@@ -4967,6 +5187,7 @@
 	unsigned long long new;
 	int err;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = kstrtoull(buf, 10, &new);
 	if (err < 0)
 		return err;
@@ -4995,6 +5216,7 @@
 static ssize_t
 reshape_position_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->reshape_position != MaxSector)
 		return sprintf(page, "%llu\n",
 			       (unsigned long long)mddev->reshape_position);
@@ -5009,6 +5231,7 @@
 	unsigned long long new;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = kstrtoull(buf, 10, &new);
 	if (err < 0)
 		return err;
@@ -5041,6 +5264,7 @@
 static ssize_t
 reshape_direction_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	return sprintf(page, "%s\n",
 		       mddev->reshape_backwards ? "backwards" : "forwards");
 }
@@ -5051,6 +5275,7 @@
 	int backwards = 0;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (cmd_match(buf, "forwards"))
 		backwards = 0;
 	else if (cmd_match(buf, "backwards"))
@@ -5082,6 +5307,7 @@
 static ssize_t
 array_size_show(struct mddev *mddev, char *page)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->external_size)
 		return sprintf(page, "%llu\n",
 			       (unsigned long long)mddev->array_sectors/2);
@@ -5095,6 +5321,7 @@
 	sector_t sectors;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = mddev_lock(mddev);
 	if (err)
 		return err;
@@ -5141,6 +5368,7 @@
 {
 	int ret;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (test_bit(MD_HAS_JOURNAL, &mddev->flags)) {
 		ret = sprintf(page, "journal\n");
 	} else if (test_bit(MD_HAS_PPL, &mddev->flags)) {
@@ -5164,6 +5392,7 @@
 {
 	int err = 0;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers) {
 		if (mddev->pers->change_consistency_policy)
 			err = mddev->pers->change_consistency_policy(mddev, buf);
@@ -5230,18 +5459,25 @@
 	struct mddev *mddev = container_of(kobj, struct mddev, kobj);
 	ssize_t rv;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!entry->show)
+{
+// pr_err("%s:%u:%s %s entry->show is NULL -EIO",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EIO;
+}
 	spin_lock(&all_mddevs_lock);
 	if (list_empty(&mddev->all_mddevs)) {
 		spin_unlock(&all_mddevs_lock);
+// pr_err("%s:%u:%s %s -EBUSY",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EBUSY;
 	}
 	mddev_get(mddev);
 	spin_unlock(&all_mddevs_lock);
 
+// pr_err("%s:%u:%s %s calling entry->show",__FILE__,__LINE__,__func__,mdname(mddev));
 	rv = entry->show(mddev, page);
 	mddev_put(mddev);
+// pr_err("%s:%u:%s %s rv=%lu",__FILE__,__LINE__,__func__,mdname(mddev),rv);
 	return rv;
 }
 
@@ -5253,8 +5489,12 @@
 	struct mddev *mddev = container_of(kobj, struct mddev, kobj);
 	ssize_t rv;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!entry->store)
+{
+pr_err("%s:%u:%s %s entry->store == NULL return -EIO",__FILE__,__LINE__,__func__, mdname(mddev));
 		return -EIO;
+}
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	spin_lock(&all_mddevs_lock);
@@ -5264,6 +5504,7 @@
 	}
 	mddev_get(mddev);
 	spin_unlock(&all_mddevs_lock);
+pr_err("%s:%u:%s %s calling entry->store",__FILE__,__LINE__,__func__, mdname(mddev));
 	rv = entry->store(mddev, page, length);
 	mddev_put(mddev);
 	return rv;
@@ -5273,6 +5514,7 @@
 {
 	struct mddev *mddev = container_of(ko, struct mddev, kobj);
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->sysfs_state)
 		sysfs_put(mddev->sysfs_state);
 
@@ -5314,6 +5556,7 @@
 
 int mddev_init_writes_pending(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->writes_pending.percpu_count_ptr)
 		return 0;
 	if (percpu_ref_init(&mddev->writes_pending, no_op,
@@ -5344,6 +5587,7 @@
 	int unit;
 	int error;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev)
 		return -ENODEV;
 
@@ -5445,6 +5689,7 @@
 
 static struct kobject *md_probe(dev_t dev, int *part, void *data)
 {
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	if (create_on_open)
 		md_alloc(dev, NULL);
 	return NULL;
@@ -5463,6 +5708,7 @@
 	char buf[DISK_NAME_LEN];
 	unsigned long devnum;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	while (len && val[len-1] == '\n')
 		len--;
 	if (len >= DISK_NAME_LEN)
@@ -5483,6 +5729,7 @@
 {
 	struct mddev *mddev = from_timer(mddev, t, safemode_timer);
 
+// pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	mddev->safemode = 1;
 	if (mddev->external)
 		sysfs_notify_dirent_safe(mddev->sysfs_state);
@@ -5498,22 +5745,36 @@
 	struct md_rdev *rdev;
 	struct md_personality *pers;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (list_empty(&mddev->disks))
+{
 		/* cannot run an array with no devices.. */
+pr_err("%s:%u:%s %s list_empty(disks) -EINVAL",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EINVAL;
+}
 
 	if (mddev->pers)
+{
+pr_err("%s:%u:%s %s pers != 0 -EBUSY",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EBUSY;
+}
 	/* Cannot run until previous stop completes properly */
 	if (mddev->sysfs_active)
+{
+pr_err("%s:%u:%s %s sysfs_active != 0 -EBUSY",__FILE__,__LINE__,__func__,mdname(mddev));
 		return -EBUSY;
+}
 
 	/*
 	 * Analyze all RAID superblock(s)
 	 */
 	if (!mddev->raid_disks) {
 		if (!mddev->persistent)
+{
+pr_err("%s:%u:%s %s raid_disks != 0 -EINVAL",__FILE__,__LINE__,__func__,mdname(mddev));
 			return -EINVAL;
+}
+pr_err("%s:%u:%s %s calling analyze_sbs",__FILE__,__LINE__,__func__,mdname(mddev));
 		analyze_sbs(mddev);
 	}
 
@@ -5556,6 +5817,7 @@
 			    > rdev->sb_start) {
 				pr_warn("md: %s: data overlaps metadata\n",
 					mdname(mddev));
+pr_err("%s:%u:%s %s #1 data overlaps metadata -EINVAL",__FILE__,__LINE__,__func__,mdname(mddev));
 				return -EINVAL;
 			}
 		} else {
@@ -5563,6 +5825,7 @@
 			    > rdev->data_offset) {
 				pr_warn("md: %s: metadata overlaps data\n",
 					mdname(mddev));
+pr_err("%s:%u:%s %s #2 data overlaps metadata -EINVAL",__FILE__,__LINE__,__func__,mdname(mddev));
 				return -EINVAL;
 			}
 		}
@@ -5572,12 +5835,18 @@
 	if (!bioset_initialized(&mddev->bio_set)) {
 		err = bioset_init(&mddev->bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
 		if (err)
+{
+pr_err("%s:%u:%s %s #3 err -EINVAL",__FILE__,__LINE__,__func__,mdname(mddev));
 			return err;
+}
 	}
 	if (!bioset_initialized(&mddev->sync_set)) {
 		err = bioset_init(&mddev->sync_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
 		if (err)
+{
+pr_err("%s:%u:%s %s #4 err -EINVAL",__FILE__,__LINE__,__func__,mdname(mddev));
 			return err;
+}
 	}
 
 	spin_lock(&pers_lock);
@@ -5747,6 +6016,7 @@
 	sysfs_notify_dirent_safe(mddev->sysfs_state);
 	sysfs_notify_dirent_safe(mddev->sysfs_action);
 	sysfs_notify(&mddev->kobj, NULL, "degraded");
+pr_err("%s:%u:%s %s #8 return 0",__FILE__,__LINE__,__func__,mdname(mddev));
 	return 0;
 
 bitmap_abort:
@@ -5759,6 +6029,7 @@
 abort:
 	bioset_exit(&mddev->bio_set);
 	bioset_exit(&mddev->sync_set);
+pr_err("%s:%u:%s %s #9 return %d",__FILE__,__LINE__,__func__,mdname(mddev),err);
 	return err;
 }
 EXPORT_SYMBOL_GPL(md_run);
@@ -5767,6 +6038,7 @@
 {
 	int err;
 
+pr_err("%s:%u:%s %s Entering and calling md_run",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = md_run(mddev);
 	if (err)
 		goto out;
@@ -5797,6 +6069,7 @@
 {
 	int ret = 0;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers->start) {
 		set_bit(MD_RECOVERY_WAIT, &mddev->recovery);
 		md_wakeup_thread(mddev->thread);
@@ -5815,6 +6088,7 @@
 	bool has_journal = false;
 	bool has_readonly = false;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* Complain if it has no devices */
 	if (list_empty(&mddev->disks))
 		return -ENXIO;
@@ -5897,10 +6171,12 @@
 	mddev->bitmap_info.daemon_sleep = 0;
 	mddev->bitmap_info.max_write_behind = 0;
 	mddev->bitmap_info.nodes = 0;
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 }
 
 static void __md_stop_writes(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	set_bit(MD_RECOVERY_FROZEN, &mddev->recovery);
 	flush_workqueue(md_misc_wq);
 	if (mddev->sync_thread) {
@@ -5930,6 +6206,7 @@
 
 void md_stop_writes(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	mddev_lock_nointr(mddev);
 	__md_stop_writes(mddev);
 	mddev_unlock(mddev);
@@ -5938,6 +6215,7 @@
 
 static void mddev_detach(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	md_bitmap_wait_behind_writes(mddev);
 	if (mddev->pers && mddev->pers->quiesce) {
 		mddev->pers->quiesce(mddev, 1);
@@ -5951,6 +6229,7 @@
 static void __md_stop(struct mddev *mddev)
 {
 	struct md_personality *pers = mddev->pers;
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	md_bitmap_destroy(mddev);
 	mddev_detach(mddev);
 	/* Ensure ->event_work is done */
@@ -5971,6 +6250,7 @@
 	/* stop the array and free an attached data structures.
 	 * This is called from dm-raid
 	 */
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	__md_stop(mddev);
 	bioset_exit(&mddev->bio_set);
 	bioset_exit(&mddev->sync_set);
@@ -5983,6 +6263,7 @@
 	int err = 0;
 	int did_freeze = 0;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!test_bit(MD_RECOVERY_FROZEN, &mddev->recovery)) {
 		did_freeze = 1;
 		set_bit(MD_RECOVERY_FROZEN, &mddev->recovery);
@@ -6047,6 +6328,7 @@
 	struct md_rdev *rdev;
 	int did_freeze = 0;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!test_bit(MD_RECOVERY_FROZEN, &mddev->recovery)) {
 		did_freeze = 1;
 		set_bit(MD_RECOVERY_FROZEN, &mddev->recovery);
@@ -6135,6 +6417,7 @@
 	struct md_rdev *rdev;
 	int err;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (list_empty(&mddev->disks))
 		return;
 
@@ -6146,6 +6429,7 @@
 	}
 	pr_cont("\n");
 
+pr_err("%s:%u:%s %s calling do_md_run",__FILE__,__LINE__,__func__,mdname(mddev));
 	err = do_md_run(mddev);
 	if (err) {
 		pr_warn("md: do_md_run() returned %d\n", err);
@@ -6171,6 +6455,7 @@
 	struct mddev *mddev;
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	pr_info("md: autorun ...\n");
 	while (!list_empty(&pending_raid_disks)) {
 		int unit;
@@ -6264,6 +6549,7 @@
 	int nr,working,insync,failed,spare;
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	nr = working = insync = failed = spare = 0;
 	rcu_read_lock();
 	rdev_for_each_rcu(rdev, mddev) {
@@ -6324,6 +6610,7 @@
 	char *ptr;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	file = kzalloc(sizeof(*file), GFP_NOIO);
 	if (!file)
 		return -ENOMEM;
@@ -6355,6 +6642,7 @@
 	mdu_disk_info_t info;
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (copy_from_user(&info, arg, sizeof(info)))
 		return -EFAULT;
 
@@ -6364,6 +6652,7 @@
 		info.major = MAJOR(rdev->bdev->bd_dev);
 		info.minor = MINOR(rdev->bdev->bd_dev);
 		info.raid_disk = rdev->raid_disk;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),info.raid_disk);
 		info.state = 0;
 		if (test_bit(Faulty, &rdev->flags))
 			info.state |= (1<<MD_DISK_FAULTY);
@@ -6380,6 +6669,7 @@
 	} else {
 		info.major = info.minor = 0;
 		info.raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),info.raid_disk);
 		info.state = (1<<MD_DISK_REMOVED);
 	}
 	rcu_read_unlock();
@@ -6396,6 +6686,7 @@
 	struct md_rdev *rdev;
 	dev_t dev = MKDEV(info->major,info->minor);
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev_is_clustered(mddev) &&
 		!(info->state & ((1 << MD_DISK_CLUSTER_ADD) | (1 << MD_DISK_CANDIDATE)))) {
 		pr_warn("%s: Cannot add to clustered mddev.\n",
@@ -6419,6 +6710,8 @@
 			struct md_rdev *rdev0
 				= list_entry(mddev->disks.next,
 					     struct md_rdev, same_set);
+pr_err("%s:%u:%s calling load_super",__FILE__,__LINE__,__func__);
+// pr_err("%s:%u:%s %s calling load_super",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
 			err = super_types[mddev->major_version]
 				.load_super(rdev, rdev0, mddev->minor_version);
 			if (err < 0) {
@@ -6466,10 +6759,14 @@
 				clear_bit(Bitmap_sync, &rdev->flags);
 			} else
 				rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			rdev->saved_raid_disk = rdev->raid_disk;
 		} else
+{
+pr_err("%s:%u:%s %s executing validate_super",__FILE__,__LINE__,__func__,mdname(mddev));
 			super_types[mddev->major_version].
 				validate_super(mddev, rdev);
+}
 		if ((info->state & (1<<MD_DISK_SYNC)) &&
 		     rdev->raid_disk != info->raid_disk) {
 			/* This was a hot-add request, but events doesn't
@@ -6523,6 +6820,7 @@
 		}
 
 		rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 		err = bind_rdev_to_array(rdev, mddev);
 
 		if (err)
@@ -6570,6 +6868,7 @@
 			rdev->raid_disk = info->raid_disk;
 		else
 			rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 
 		if (rdev->raid_disk < mddev->raid_disks)
 			if (info->state & (1<<MD_DISK_SYNC))
@@ -6602,6 +6901,7 @@
 	char b[BDEVNAME_SIZE];
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->pers)
 		return -ENODEV;
 
@@ -6643,6 +6943,7 @@
 	int err;
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->pers)
 		return -ENODEV;
 
@@ -6691,6 +6992,7 @@
 	 */
 
 	rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 
 	set_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);
 	if (!mddev->thread)
@@ -6713,6 +7015,7 @@
 {
 	int err = 0;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers) {
 		if (!mddev->pers->quiesce || !mddev->thread)
 			return -EBUSY;
@@ -6809,6 +7112,7 @@
 static int set_array_info(struct mddev *mddev, mdu_array_info_t *info)
 {
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (info->raid_disks == 0) {
 		/* just setting version number for superblock loading */
 		if (info->major_version < 0 ||
@@ -6880,6 +7184,7 @@
 
 void md_set_array_sectors(struct mddev *mddev, sector_t array_sectors)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	lockdep_assert_held(&mddev->reconfig_mutex);
 
 	if (mddev->external_size)
@@ -6896,6 +7201,7 @@
 	int fit = (num_sectors == 0);
 	sector_t old_dev_sectors = mddev->dev_sectors;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers->resize == NULL)
 		return -EINVAL;
 	/* The "num_sectors" is the number of sectors of each device that
@@ -6937,6 +7243,7 @@
 {
 	int rv;
 	struct md_rdev *rdev;
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* change the number of raid disks */
 	if (mddev->pers->check_reshape == NULL)
 		return -EINVAL;
@@ -6987,6 +7294,7 @@
 	int cnt = 0;
 	int state = 0;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* calculate expected state,ignoring low bits */
 	if (mddev->bitmap && mddev->bitmap_info.offset)
 		state |= (1 << MD_SB_BITMAP_PRESENT);
@@ -7111,6 +7419,7 @@
 	struct md_rdev *rdev;
 	int err = 0;
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->pers == NULL)
 		return -ENODEV;
 
@@ -7137,6 +7446,7 @@
 {
 	struct mddev *mddev = bdev->bd_disk->private_data;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	geo->heads = 2;
 	geo->sectors = 4;
 	geo->cylinders = mddev->array_sectors / 8;
@@ -7178,6 +7488,7 @@
 	int ro;
 	bool did_set_md_closing = false;
 
+pr_err("%s:%u:%s %s Entering cmd=0x%x",__FILE__,__LINE__,__func__,mdname(mddev),cmd);
 	if (!md_ioctl_valid(cmd))
 		return -ENOTTY;
 
@@ -7197,12 +7508,14 @@
 	 */
 	switch (cmd) {
 	case RAID_VERSION:
+pr_err("%s:%u:%s %s RAID_VERSION",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = get_version(argp);
 		goto out;
 
 #ifndef MODULE
 	case RAID_AUTORUN:
 		err = 0;
+pr_err("%s:%u:%s calling autostart_arrays",__FILE__,__LINE__,__func__);
 		autostart_arrays(arg);
 		goto out;
 #endif
@@ -7216,6 +7529,7 @@
 	mddev = bdev->bd_disk->private_data;
 
 	if (!mddev) {
+pr_err("%s:%u:%s %s NO MD DEV",__FILE__,__LINE__,__func__,mdname(mddev));
 		BUG();
 		goto out;
 	}
@@ -7223,6 +7537,7 @@
 	/* Some actions do not requires the mutex */
 	switch (cmd) {
 	case GET_ARRAY_INFO:
+pr_err("%s:%u:%s %s GET_ARRAY_INFO",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (!mddev->raid_disks && !mddev->external)
 			err = -ENODEV;
 		else
@@ -7230,6 +7545,7 @@
 		goto out;
 
 	case GET_DISK_INFO:
+pr_err("%s:%u:%s %s GET_DISK_INFO",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (!mddev->raid_disks && !mddev->external)
 			err = -ENODEV;
 		else
@@ -7237,29 +7553,38 @@
 		goto out;
 
 	case SET_DISK_FAULTY:
+pr_err("%s:%u:%s %s SET_DISK_FAULTY",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = set_disk_faulty(mddev, new_decode_dev(arg));
 		goto out;
 
 	case GET_BITMAP_FILE:
+pr_err("%s:%u:%s %s GET_BITMAP_FILE",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = get_bitmap_file(mddev, argp);
 		goto out;
 
 	}
 
 	if (cmd == ADD_NEW_DISK)
+{
+pr_err("%s:%u:%s %s ADD_NEW_DISK #1",__FILE__,__LINE__,__func__,mdname(mddev));
 		/* need to ensure md_delayed_delete() has completed */
 		flush_workqueue(md_misc_wq);
+}
 
 	if (cmd == HOT_REMOVE_DISK)
+{
+pr_err("%s:%u:%s %s HOT_REMOVE_DISK #1",__FILE__,__LINE__,__func__,mdname(mddev));
 		/* need to ensure recovery thread has run */
 		wait_event_interruptible_timeout(mddev->sb_wait,
 						 !test_bit(MD_RECOVERY_NEEDED,
 							   &mddev->recovery),
 						 msecs_to_jiffies(5000));
+}
 	if (cmd == STOP_ARRAY || cmd == STOP_ARRAY_RO) {
 		/* Need to flush page cache, and ensure no-one else opens
 		 * and writes
 		 */
+pr_err("%s:%u:%s %s STOP_ARRAY or STOP_ARRAY_RO",__FILE__,__LINE__,__func__,mdname(mddev));
 		mutex_lock(&mddev->open_mutex);
 		if (mddev->pers && atomic_read(&mddev->openers) > 1) {
 			mutex_unlock(&mddev->open_mutex);
@@ -7281,6 +7606,7 @@
 
 	if (cmd == SET_ARRAY_INFO) {
 		mdu_array_info_t info;
+pr_err("%s:%u:%s %s SET_ARRAY_INFO",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (!arg)
 			memset(&info, 0, sizeof(info));
 		else if (copy_from_user(&info, argp, sizeof(info))) {
@@ -7288,6 +7614,7 @@
 			goto unlock;
 		}
 		if (mddev->pers) {
+pr_err("%s:%u:%s %s mddev->pers",__FILE__,__LINE__,__func__,mdname(mddev));
 			err = update_array_info(mddev, &info);
 			if (err) {
 				pr_warn("md: couldn't update array info. %d\n", err);
@@ -7296,11 +7623,13 @@
 			goto unlock;
 		}
 		if (!list_empty(&mddev->disks)) {
+pr_err("%s:%u:%s %s already has disks!",__FILE__,__LINE__,__func__,mdname(mddev));
 			pr_warn("md: array %s already has disks!\n", mdname(mddev));
 			err = -EBUSY;
 			goto unlock;
 		}
 		if (mddev->raid_disks) {
+pr_err("%s:%u:%s %s already initialized!",__FILE__,__LINE__,__func__,mdname(mddev));
 			pr_warn("md: array %s already initialised!\n", mdname(mddev));
 			err = -EBUSY;
 			goto unlock;
@@ -7322,6 +7651,7 @@
 	    && cmd != ADD_NEW_DISK && cmd != STOP_ARRAY
 	    && cmd != RUN_ARRAY && cmd != SET_BITMAP_FILE
 	    && cmd != GET_BITMAP_FILE) {
+pr_err("%s:%u:%s %s no device",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = -ENODEV;
 		goto unlock;
 	}
@@ -7331,28 +7661,34 @@
 	 */
 	switch (cmd) {
 	case RESTART_ARRAY_RW:
+pr_err("%s:%u:%s %s RESTART_ARRAY_RW",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = restart_array(mddev);
 		goto unlock;
 
 	case STOP_ARRAY:
+pr_err("%s:%u:%s %s STOP_ARRAY",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = do_md_stop(mddev, 0, bdev);
 		goto unlock;
 
 	case STOP_ARRAY_RO:
+pr_err("%s:%u:%s %s STOP_ARRAY_RO",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = md_set_readonly(mddev, bdev);
 		goto unlock;
 
 	case HOT_REMOVE_DISK:
+pr_err("%s:%u:%s %s HOT_REMOVE_DISK",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = hot_remove_disk(mddev, new_decode_dev(arg));
 		goto unlock;
 
 	case ADD_NEW_DISK:
+pr_err("%s:%u:%s %s ADD_NEW_DISK",__FILE__,__LINE__,__func__,mdname(mddev));
 		/* We can support ADD_NEW_DISK on read-only arrays
 		 * only if we are re-adding a preexisting device.
 		 * So require mddev->pers and MD_DISK_SYNC.
 		 */
 		if (mddev->pers) {
 			mdu_disk_info_t info;
+pr_err("%s:%u:%s %s ADD_NEW_DISK - mddev->pers",__FILE__,__LINE__,__func__,mdname(mddev));
 			if (copy_from_user(&info, argp, sizeof(info)))
 				err = -EFAULT;
 			else if (!(info.state & (1<<MD_DISK_SYNC)))
@@ -7365,6 +7701,7 @@
 		break;
 
 	case BLKROSET:
+pr_err("%s:%u:%s %s BLKROSET",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (get_user(ro, (int __user *)(arg))) {
 			err = -EFAULT;
 			goto unlock;
@@ -7399,6 +7736,7 @@
 	 * superblock, so we do not allow them on read-only arrays.
 	 */
 	if (mddev->ro && mddev->pers) {
+pr_err("%s:%u:%s %s REST OF IOCTLS RO CHECK",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (mddev->ro == 2) {
 			mddev->ro = 0;
 			sysfs_notify_dirent_safe(mddev->sysfs_state);
@@ -7424,6 +7762,7 @@
 	case ADD_NEW_DISK:
 	{
 		mdu_disk_info_t info;
+pr_err("%s:%u:%s %s ADD_NEW_DISK",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (copy_from_user(&info, argp, sizeof(info)))
 			err = -EFAULT;
 		else
@@ -7432,6 +7771,7 @@
 	}
 
 	case CLUSTERED_DISK_NACK:
+pr_err("%s:%u:%s %s CLUSTERED_DISK_NACK",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (mddev_is_clustered(mddev))
 			md_cluster_ops->new_disk_ack(mddev, false);
 		else
@@ -7439,18 +7779,22 @@
 		goto unlock;
 
 	case HOT_ADD_DISK:
+pr_err("%s:%u:%s %s HOT_ADD_DISK",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = hot_add_disk(mddev, new_decode_dev(arg));
 		goto unlock;
 
 	case RUN_ARRAY:
+pr_err("%s:%u:%s %s RUN_ARRAY calling do_md_run",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = do_md_run(mddev);
 		goto unlock;
 
 	case SET_BITMAP_FILE:
+pr_err("%s:%u:%s %s SET_BITMAP_FILE",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = set_bitmap_file(mddev, (int)arg);
 		goto unlock;
 
 	default:
+pr_err("%s:%u:%s %s DEFAULT, error",__FILE__,__LINE__,__func__,mdname(mddev));
 		err = -EINVAL;
 		goto unlock;
 	}
@@ -7497,6 +7841,7 @@
 	if (!mddev)
 		return -ENODEV;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->gendisk != bdev->bd_disk) {
 		/* we are racing with mddev_put which is discarding this
 		 * bd_disk.
@@ -7533,6 +7878,7 @@
 {
 	struct mddev *mddev = disk->private_data;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	BUG_ON(!mddev);
 	atomic_dec(&mddev->openers);
 	mddev_put(mddev);
@@ -7542,6 +7888,7 @@
 {
 	struct mddev *mddev = disk->private_data;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	return mddev->changed;
 }
 
@@ -7549,6 +7896,7 @@
 {
 	struct mddev *mddev = disk->private_data;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	mddev->changed = 0;
 	return 0;
 }
@@ -7624,6 +7972,7 @@
 {
 	struct md_thread *thread;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	thread = kzalloc(sizeof(struct md_thread), GFP_KERNEL);
 	if (!thread)
 		return NULL;
@@ -7665,6 +8014,7 @@
 
 void md_error(struct mddev *mddev, struct md_rdev *rdev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!rdev || test_bit(Faulty, &rdev->flags))
 		return;
 
@@ -7690,6 +8040,7 @@
 	int i = 0;
 	struct md_rdev *rdev;
 
+// pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	seq_printf(seq, "unused devices: ");
 
 	list_for_each_entry(rdev, &pending_raid_disks, same_set) {
@@ -7712,6 +8063,7 @@
 	int scale, recovery_active;
 	unsigned int per_milli;
 
+// pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ||
 	    test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))
 		max_sectors = mddev->resync_max_sectors;
@@ -7838,6 +8190,7 @@
 	loff_t l = *pos;
 	struct mddev *mddev;
 
+// pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	if (l >= 0x10000)
 		return NULL;
 	if (!l--)
@@ -7848,6 +8201,7 @@
 	list_for_each(tmp,&all_mddevs)
 		if (!l--) {
 			mddev = list_entry(tmp, struct mddev, all_mddevs);
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 			mddev_get(mddev);
 			spin_unlock(&all_mddevs_lock);
 			return mddev;
@@ -7863,6 +8217,7 @@
 	struct list_head *tmp;
 	struct mddev *next_mddev, *mddev = v;
 
+// pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	++*pos;
 	if (v == (void*)2)
 		return NULL;
@@ -7900,6 +8255,7 @@
 	sector_t sectors;
 	struct md_rdev *rdev;
 
+// char *p = seq->buf;
 	if (v == (void*)1) {
 		struct md_personality *pers;
 		seq_printf(seq, "Personalities : ");
@@ -7910,13 +8266,17 @@
 		spin_unlock(&pers_lock);
 		seq_printf(seq, "\n");
 		seq->poll_event = atomic_read(&md_event_count);
+// pr_err("%s:%u:%s #1 seq->buf='%s'",__FILE__,__LINE__,__func__,p);
+pr_err("%s:%u:%s #1",__FILE__,__LINE__,__func__);
 		return 0;
 	}
 	if (v == (void*)2) {
 		status_unused(seq);
+// pr_err("%s:%u:%s #2",__FILE__,__LINE__,__func__);
 		return 0;
 	}
 
+// pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	spin_lock(&mddev->lock);
 	if (mddev->pers || mddev->raid_disks || !list_empty(&mddev->disks)) {
 		seq_printf(seq, "%s : %sactive", mdname(mddev),
@@ -7988,6 +8348,7 @@
 		seq_printf(seq, "\n");
 	}
 	spin_unlock(&mddev->lock);
+// pr_err("%s:%u:%s %s seq->buf='%s'",__FILE__,__LINE__,__func__,mdname(mddev),p);
 
 	return 0;
 }
@@ -8004,6 +8365,7 @@
 	struct seq_file *seq;
 	int error;
 
+// pr_err("%s:%u:%s file='%s'",__FILE__,__LINE__,__func__,file->f_path.dentry->d_iname);
 	error = seq_open(file, &md_seq_ops);
 	if (error)
 		return error;
@@ -8021,6 +8383,7 @@
 
 	if (md_unloading)
 		return EPOLLIN|EPOLLRDNORM|EPOLLERR|EPOLLPRI;
+pr_err("%s:%u:%s file='%s'",__FILE__,__LINE__,__func__,filp->f_path.dentry->d_iname);
 	poll_wait(filp, &md_event_waiters, wait);
 
 	/* always allow read */
@@ -8053,6 +8416,7 @@
 
 int unregister_md_personality(struct md_personality *p)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,p->name);
 	pr_debug("md: %s personality unregistered\n", p->name);
 	spin_lock(&pers_lock);
 	list_del_init(&p->list);
@@ -8088,6 +8452,7 @@
 
 int md_setup_cluster(struct mddev *mddev, int nodes)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!md_cluster_ops)
 		request_module("md-cluster");
 	spin_lock(&pers_lock);
@@ -8104,6 +8469,7 @@
 
 void md_cluster_stop(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!md_cluster_ops)
 		return;
 	md_cluster_ops->leave(mddev);
@@ -8116,6 +8482,7 @@
 	int idle;
 	int curr_events;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	idle = 1;
 	rcu_read_lock();
 	rdev_for_each_rcu(rdev, mddev) {
@@ -8155,6 +8522,7 @@
 
 void md_done_sync(struct mddev *mddev, int blocks, int ok)
 {
+// pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* another "blocks" (512byte) blocks have been synced */
 	atomic_sub(blocks, &mddev->recovery_active);
 	wake_up(&mddev->recovery_wait);
@@ -8178,6 +8546,7 @@
 {
 	int did_change = 0;
 
+// pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (bio_data_dir(bi) != WRITE)
 		return true;
 
@@ -8233,6 +8602,7 @@
  */
 void md_write_inc(struct mddev *mddev, struct bio *bi)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (bio_data_dir(bi) != WRITE)
 		return;
 	WARN_ON_ONCE(mddev->in_sync || mddev->ro);
@@ -8242,6 +8612,7 @@
 
 void md_write_end(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	percpu_ref_put(&mddev->writes_pending);
 
 	if (mddev->safemode == 2)
@@ -8265,6 +8636,7 @@
  */
 void md_allow_write(struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mddev->pers)
 		return;
 	if (mddev->ro)
@@ -8312,6 +8684,7 @@
 	struct blk_plug plug;
 	int ret;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* just incase thread restarts... */
 	if (test_bit(MD_RECOVERY_DONE, &mddev->recovery) ||
 	    test_bit(MD_RECOVERY_WAIT, &mddev->recovery))
@@ -8722,6 +9095,7 @@
 	int removed = 0;
 	bool remove_some = false;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (this && test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))
 		/* Mustn't remove devices when resync thread is running */
 		return 0;
@@ -8757,6 +9131,7 @@
 				sysfs_unlink_rdev(mddev, rdev);
 				rdev->saved_raid_disk = rdev->raid_disk;
 				rdev->raid_disk = -1;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 				removed++;
 			}
 		}
@@ -8812,6 +9187,7 @@
 {
 	struct mddev *mddev = container_of(ws, struct mddev, del_work);
 
+pr_err("%s:%u:%s %s Entering",__FILE__,__LINE__,__func__,mdname(mddev));
 	mddev->sync_thread = md_register_thread(md_do_sync,
 						mddev,
 						"resync");
@@ -8859,6 +9235,7 @@
  */
 void md_check_recovery(struct mddev *mddev)
 {
+// pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (test_bit(MD_ALLOW_SB_UPDATE, &mddev->flags) && mddev->sb_flags) {
 		/* Write superblock - thread that called mddev_suspend()
 		 * holds reconfig_mutex for us.
@@ -9040,6 +9417,7 @@
 	sector_t old_dev_sectors = mddev->dev_sectors;
 	bool is_reshaped = false;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* resync has finished, collect result */
 	md_unregister_thread(&mddev->sync_thread);
 	if (!test_bit(MD_RECOVERY_INTR, &mddev->recovery) &&
@@ -9098,6 +9476,7 @@
 
 void md_wait_for_blocked_rdev(struct md_rdev *rdev, struct mddev *mddev)
 {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	sysfs_notify_dirent_safe(rdev->sysfs_state);
 	wait_event_timeout(rdev->blocked_wait,
 			   !test_bit(Blocked, &rdev->flags) &&
@@ -9112,6 +9491,7 @@
 	/* called be personality module when reshape completes. */
 	struct md_rdev *rdev;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	rdev_for_each(rdev, mddev) {
 		if (rdev->data_offset > rdev->new_data_offset)
 			rdev->sectors += rdev->data_offset - rdev->new_data_offset;
@@ -9130,6 +9510,7 @@
 {
 	struct mddev *mddev = rdev->mddev;
 	int rv;
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (is_new)
 		s += rdev->new_data_offset;
 	else
@@ -9154,6 +9535,7 @@
 			 int is_new)
 {
 	int rv;
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	if (is_new)
 		s += rdev->new_data_offset;
 	else
@@ -9173,6 +9555,7 @@
 	int need_delay = 0;
 
 	for_each_mddev(mddev, tmp) {
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 		if (mddev_trylock(mddev)) {
 			if (mddev->pers)
 				__md_stop_writes(mddev);
@@ -9211,6 +9594,7 @@
 {
 	int ret = -ENOMEM;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	md_wq = alloc_workqueue("md", WQ_MEM_RECLAIM, 0);
 	if (!md_wq)
 		goto err_wq;
@@ -9254,6 +9638,7 @@
 	int role, ret;
 	char b[BDEVNAME_SIZE];
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/*
 	 * If size is changed in another node then we need to
 	 * do resize as well.
@@ -9288,6 +9673,7 @@
 			/*
 			 * got activated except reshape is happening.
 			 */
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev2->raid_disk);
 			if (rdev2->raid_disk == -1 && role != 0xffff &&
 			    !(le32_to_cpu(sb->feature_map) &
 			      MD_FEATURE_RESHAPE_ACTIVE)) {
@@ -9349,6 +9735,7 @@
 	struct page *swapout = rdev->sb_page;
 	struct mdp_superblock_1 *sb;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* Store the sb page of the rdev in the swapout temporary
 	 * variable in case we err in the future
 	 */
@@ -9357,6 +9744,8 @@
 	if (err == 0) {
 		ClearPageUptodate(rdev->sb_page);
 		rdev->sb_loaded = 0;
+// pr_err("%s:%u:%s %s calling load_super",__FILE__,__LINE__,__func__,mdname(rdev->mddev));
+pr_err("%s:%u:%s calling load_super",__FILE__,__LINE__,__func__);
 		err = super_types[mddev->major_version].
 			load_super(rdev, NULL, mddev->minor_version);
 	}
@@ -9395,6 +9784,7 @@
 	struct md_rdev *rdev;
 	int err;
 
+pr_err("%s:%u:%s %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/* Find the rdev */
 	rdev_for_each_rcu(rdev, mddev) {
 		if (rdev->desc_nr == nr)
@@ -9438,6 +9828,7 @@
 {
 	struct detected_devices_node *node_detected_dev;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	node_detected_dev = kzalloc(sizeof(*node_detected_dev), GFP_KERNEL);
 	if (node_detected_dev) {
 		node_detected_dev->dev = dev;
@@ -9454,6 +9845,7 @@
 	dev_t dev;
 	int i_scanned, i_passed;
 
+pr_err("%s:%u:%s Entering",__FILE__,__LINE__,__func__);
 	i_scanned = 0;
 	i_passed = 0;
 
diff --no-dereference -ru ORIG.linux/drivers/md/md.h linux/drivers/md/md.h
--- ORIG.linux/drivers/md/md.h	2019-09-15 16:19:32.000000000 -0500
+++ linux/drivers/md/md.h	2022-03-14 11:29:58.945283527 -0500
@@ -610,7 +610,7 @@
 
 static inline char * mdname (struct mddev * mddev)
 {
-	return mddev->gendisk ? mddev->gendisk->disk_name : "mdX";
+	return (mddev && mddev->gendisk && mddev->gendisk->disk_name) ? mddev->gendisk->disk_name : "mdX";
 }
 
 static inline int sysfs_link_rdev(struct mddev *mddev, struct md_rdev *rdev)
diff --no-dereference -ru ORIG.linux/drivers/md/raid1.c linux/drivers/md/raid1.c
--- ORIG.linux/drivers/md/raid1.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/drivers/md/raid1.c	2022-04-20 14:46:26.625765538 -0500
@@ -1602,6 +1602,7 @@
 	}
 	rcu_read_unlock();
 	seq_printf(seq, "]");
+// pr_err("%s:%u:%s %s use seq_file",__FILE__,__LINE__,__func__, mdname(mddev));
 }
 
 static void raid1_error(struct mddev *mddev, struct md_rdev *rdev)
@@ -1610,6 +1611,7 @@
 	struct r1conf *conf = mddev->private;
 	unsigned long flags;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/*
 	 * If it is not operational, then we have already marked it as dead
 	 * else if it is the last working disks, ignore the error, let the
@@ -1690,6 +1692,7 @@
 	int count = 0;
 	unsigned long flags;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	/*
 	 * Find all failed disks within the RAID1 configuration
 	 * and mark them readable.
@@ -1744,6 +1747,7 @@
 	int first = 0;
 	int last = conf->raid_disks - 1;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->recovery_disabled == conf->recovery_disabled)
 		return -EBUSY;
 
@@ -1772,6 +1776,7 @@
 
 			p->head_position = 0;
 			rdev->raid_disk = mirror;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			err = 0;
 			/* As all devices are equivalent, we don't need a full recovery
 			 * if this was recently any drive of the array
@@ -1787,6 +1792,7 @@
 			clear_bit(In_sync, &rdev->flags);
 			set_bit(Replacement, &rdev->flags);
 			rdev->raid_disk = mirror;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			err = 0;
 			conf->fullsync = 1;
 			rcu_assign_pointer(p[conf->raid_disks].rdev, rdev);
@@ -1806,6 +1812,7 @@
 	int number = rdev->raid_disk;
 	struct raid1_info *p = conf->mirrors + number;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (rdev != p->rdev)
 		p = conf->mirrors + conf->raid_disks + number;
 
@@ -2645,6 +2652,7 @@
 	int idx = sector_to_idx(sector_nr);
 	int page_idx = 0;
 
+// pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (!mempool_initialized(&conf->r1buf_pool))
 		if (init_resync(conf))
 			return 0;
@@ -2928,6 +2936,7 @@
 
 static sector_t raid1_size(struct mddev *mddev, sector_t sectors, int raid_disks)
 {
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (sectors)
 		return sectors;
 
@@ -3082,6 +3091,7 @@
 	int ret;
 	bool discard_supported = false;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->level != 1) {
 		pr_warn("md/raid1:%s: raid level not set to mirroring (%d)\n",
 			mdname(mddev), mddev->level);
@@ -3169,6 +3179,7 @@
 {
 	struct r1conf *conf = priv;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	mempool_exit(&conf->r1bio_pool);
 	kfree(conf->mirrors);
 	safe_put_page(conf->tmppage);
@@ -3191,6 +3202,7 @@
 	 * worth it.
 	 */
 	sector_t newsize = raid1_size(mddev, sectors, 0);
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->external_size &&
 	    mddev->array_sectors > newsize)
 		return -EINVAL;
@@ -3232,6 +3244,7 @@
 	int d, d2;
 	int ret;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	memset(&newpool, 0, sizeof(newpool));
 	memset(&oldpool, 0, sizeof(oldpool));
 
@@ -3291,6 +3304,7 @@
 		if (rdev && rdev->raid_disk != d2) {
 			sysfs_unlink_rdev(mddev, rdev);
 			rdev->raid_disk = d2;
+pr_err("%s:%u:%s %s raid_disk=%d",__FILE__,__LINE__,__func__,mdname(mddev),rdev->raid_disk);
 			sysfs_unlink_rdev(mddev, rdev);
 			if (sysfs_link_rdev(mddev, rdev))
 				pr_warn("md/raid1:%s: cannot register rd%d\n",
@@ -3324,6 +3338,7 @@
 {
 	struct r1conf *conf = mddev->private;
 
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (quiesce)
 		freeze_array(conf, 0);
 	else
@@ -3335,6 +3350,7 @@
 	/* raid1 can take over:
 	 *  raid5 with 2 devices, any layout or chunk size
 	 */
+pr_err("%s:%u:%s Entering %s",__FILE__,__LINE__,__func__,mdname(mddev));
 	if (mddev->level == 5 && mddev->raid_disks == 2) {
 		struct r1conf *conf;
 		mddev->new_level = 1;
diff --no-dereference -ru ORIG.linux/fs/exec.c linux/fs/exec.c
--- ORIG.linux/fs/exec.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/fs/exec.c	2022-04-22 07:37:38.964575372 -0500
@@ -1802,7 +1802,6 @@
 	retval = prepare_binprm(bprm);
 	if (retval < 0)
 		goto out;
-
 	retval = copy_strings_kernel(1, &bprm->filename, bprm);
 	if (retval < 0)
 		goto out;
@@ -1816,6 +1815,87 @@
 	if (retval < 0)
 		goto out;
 
+#if 1
+{
+  static int hundred = 0;
+//   if (hundred < 16000 &&
+//    strcmp(bprm->filename, "/lib64/ld-linux-x86-64.so.2") != 0 &&
+//     strcmp(bprm->filename, "/sbin/modprobe") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/basename") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/cat") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/cp") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/cpio") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/cut") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/date") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/df") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/egrep") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/find") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/findmnt") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/grep") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/gzip") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/head") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/journalctl") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/ldd") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/ln") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/mkdir") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/python3") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/readlink") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/rmdir") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/sed") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/sort") != 0 &&
+//     strcmp(bprm->filename, "/usr/bin/systemctl") != 0 &&
+//     strcmp(bprm->filename, "/usr/libexec/microcode_ctl/check_caveats") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/biosdevname") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/blkid") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/ethtool") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/ip") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/iscsiadm") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/ldconfig") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/lldpad") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/lldptool") != 0 &&
+//     strcmp(bprm->filename, "/usr/sbin/modinfo") != 0)
+  if (hundred < 16000)
+  {
+    int i;
+    int j;
+    int l;
+    char args[1024];
+    hundred++;
+    memset(args, 0, sizeof(args));
+    l = snprintf(args, 1024, "EXECVE %s", bprm->filename);
+#if 1
+    for (i = 1; i < bprm->argc; i++)
+    {
+      const char *p = get_user_arg_ptr(argv,i);
+      if (!p)
+      {
+	  j = snprintf(args + l, 1024 - l, " NULL");
+      }
+      else
+      {
+	  char k[512];
+	  memset(k, 0, sizeof(k));
+	  j = strncpy_from_user(k, p, 512);
+	  if (j < 0)
+	  {
+	      j = snprintf(k, 100, "Err=%d", j);
+	  }
+	  k[510] = k[511] = '\0';
+	  j = snprintf(args + l, 1024 - l, " %s", k);
+      }
+      l = l + j;
+      if (l >= 1024)
+      {
+	break;
+      }
+    }
+#endif	/* 1 */
+    args[1022] = args[1023] = '\0';
+    printk("%s:%u:%s %s", __FILE__,__LINE__,__func__, args);
+  }
+}
+#endif	/* 1 */
+
 	would_dump(bprm, bprm->file);
 
 	retval = exec_binprm(bprm);
diff --no-dereference -ru ORIG.linux/kernel/printk/printk.c linux/kernel/printk/printk.c
--- ORIG.linux/kernel/printk/printk.c	2019-09-15 16:19:32.000000000 -0500
+++ linux/kernel/printk/printk.c	2022-03-23 13:09:44.214519596 -0500
@@ -112,7 +112,8 @@
 };
 
 /* Keep both the 'on' and 'off' bits clear, i.e. ratelimit by default: */
-#define DEVKMSG_LOG_MASK_DEFAULT	0
+// #define DEVKMSG_LOG_MASK_DEFAULT	0
+#define DEVKMSG_LOG_MASK_DEFAULT	1
 
 static unsigned int __read_mostly devkmsg_log = DEVKMSG_LOG_MASK_DEFAULT;
 
